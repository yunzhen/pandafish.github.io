<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>从0开始部署一个Flink集群：理论篇</title>
    <link href="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/"/>
    <url>/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<p>本系列博文由3篇文章组成：</p><ul><li><a href="https://yunzhen.github.io/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/">从0开始部署一个Flink集群：理论篇</a></li><li><a href="https://yunzhen.github.io/2022/12/18/Flink%E9%83%A8%E7%BD%B2%E7%8B%AC%E7%AB%8B%E9%83%A8%E7%BD%B2%E7%AF%87/">从0开始部署一个Flink集群：实践篇（独立部署）</a></li><li><a href="https://yunzhen.github.io/2022/12/19/Flink%E9%83%A8%E7%BD%B2k8s%E7%AF%87/">从0开始部署一个Flink集群：实践篇（Native k8s部署）</a></li></ul><p>主要回答以下问题：</p><ul><li>Flink集群是由哪些组件组成的？它们彼此之间如何协调工作的？</li><li>在Flink中job, task, slots,parallelism是什么意思？集群中的资源是如何调度和分配的？</li><li>如何搭建一个Flink集群？如何配置高可用服务？如何使用外部文件系统？</li></ul><h1 id="Flink系统架构"><a href="#Flink系统架构" class="headerlink" title="Flink系统架构"></a>Flink系统架构</h1><p><img src="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/2022-12-21-20-36-32.png"></p><p>Flink的核心组件包含客户端，jobmanager（JM）和taskmanager(TM)三部分。此外Flink往往还需要结合很多外部组件一起使用，比如高可用服务、持久化存储、资源管理、指标存储与分析的组件。</p><p>Flink客户端主要负责将job提交给JM。JM是中央调度器，包含Jobmaster, Dispatcher, ResourceManager三部分。<strong>JobMaster</strong> is responsible for managing the execution of a single JobGraph. Multiple jobs can run simultaneously in a Flink cluster, each having its own JobMaster. <strong>The Dispatcher</strong> provides a REST interface to submit Flink applications for execution and starts a new JobMaster for each submitted job. It also runs the Flink WebUI to provide information about job executions. <strong>The ResourceManager</strong> is responsible for resource de-&#x2F;allocation and provisioning in a Flink cluster — it manages task slots, which are the unit of resource scheduling in a Flink cluster. TM负责执行具体的任务。</p><p>如果只是提交作业和执行作业，不考虑整个集群的稳定性，拓展性，便于维护的性能等，只部署以上三个组件就够了。</p><p>但是，如果TM done掉了，JM还可以控制任务重启在其它TM上；如果JM done掉了，所有的任务都将失败，因此我们需要部署<strong>高可用服务</strong>使得一个JM done掉后，备用的JM 自动地顶上去作业。Flink目前（1.16）仅支持两种高可用服务：Zookeeper HA service 和 K8s HA service.</p><p>Flink有<strong>故障恢复</strong>的机制在任务失败后重启任务，并读取任务失败前的状态在这个状态下继续工作，可以保证哪怕任务失败重启，数据也不丢失，不重发。而这个“任务失败前的状态”是通过checkpoint保存的，考虑到多个JM需要共享checkpoint，checkpoint往往保存在可共享的持久化外部存储系统中，比如HDFS，S3等。因此我们还需要部署文件存储系统。</p><p>再说集群的资源管理和调度，Flink支持k8s和YARN两种工具来自动化管理集群资源，也可以不依赖于任何<strong>Resource Provider</strong>，采用独立部署（standalone）方式部署集群。</p><p>再说集群的监控，Flink本身收集了很多指标，可以通过<strong>metrics reporter</strong>与外部的指标存储、分析、展示工具一起搭建一个Flink监控系统。比如联合Prometheus, grafana搭建监控系统。</p><h1 id="Flink的作业执行机制"><a href="#Flink的作业执行机制" class="headerlink" title="Flink的作业执行机制"></a>Flink的作业执行机制</h1><p>在讲解Flink不同的部署方式以及不同部署方式下各组件如何协调工作前，我认为很有必要讲解一下Flink的作业执行机制，便于理解之后会反复提到的JobGraph，task, slots等概念。</p><h2 id="DataFlows和Operator"><a href="#DataFlows和Operator" class="headerlink" title="DataFlows和Operator"></a>DataFlows和Operator</h2><p>程序运行时会被映射为dataflows,每个数据流都是以一个或多个sources开始，一个或多个sinks结束，类似于任意的有向无环图。大多数情况下，程序中的转换运算和dataflow中的算子（operator)是一一对应的关系。</p><p>比如下图中的程序就可以转化为由source，map算子，分组聚合算子，sink组成的数据流。<br><img src="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/2022-12-21-21-47-01.png"></p><h2 id="并行计算和并行度（Parallelism"><a href="#并行计算和并行度（Parallelism" class="headerlink" title="并行计算和并行度（Parallelism)"></a>并行计算和并行度（Parallelism)</h2><p><img src="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/2022-12-21-21-49-48.png"></p><p>任务并行：不同的任务（算子）并行处理不同的数据，数据流图中横向的同时执行。</p><p>数据并行：一个算子可以包含一个或多个子任务，这些子任务在不同的线程、不同的物理机或容器中完全独立地执行。</p><p>并行度：一个特定<strong>算子</strong>的子任务个数，指的数据并行。有些像多线程的线程数，但和多线程不一样的是，多线程的子线程共享内存资源，但是一个算子的子任务运行在不同的slot上，内存资源是隔离的。注意并行度针对的是算子，不同的算子可以设置为不同的并行度。</p><p>并行度的设置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//全局，不推荐</span><br>env.setParallelism(<span class="hljs-number">1</span>);<br><span class="hljs-comment">//每一个算子</span><br>source.map(...).setParallelism(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>并行度的执行规则：底层实现&gt;代码局部&gt;代码全局设置&gt;提交任务时的命令行设置&gt;配置文件的默认设置</p><h2 id="算子链"><a href="#算子链" class="headerlink" title="算子链"></a>算子链</h2><p>上面介绍了数据流图，算子，并行度的概念，再来说什么是算子链。</p><p>Flink中算子与算子之间的数据传输形式大体可以分为以下两类：</p><ul><li><p>one-to-one(forwarding) 直通：</p><p>  从一个算子到另一个算子的分区不变，比如source和map之间，这代表着map算子的子任务看到的元素的个数和顺序和source算子的子任务产生的相同。map,filter,flatMap都属于这种（前提是并行度不变）</p></li><li><p>redistributing(重分配）：</p><p>  stream的分区会发生改变，如keyby.</p></li></ul><p>如果前后两个算子并行度相同，且传输方式为one-to-one就可以合并为一个算子链。通常我们说的<strong>task</strong>就是指的一个算子链，<strong>subtask</strong>往往指的同一算子链的子任务。</p><p>算子合并为算子链是作业执行中很重要的一个优化手段，是否合并是可以通过代码控制的，在作业的性能调优中也是一个可以考虑的调优点。</p><p>Flink中之所以合并算子主要考虑的是减少算子之间不必要的数据传输，因为在flink中，不同任务之间的数据传输带来的性能开销其实并不小，一是数据传输必然涉及到序列化和反序列，要是一条数据很大，又选择了不合适的数据类型比如json，那带来的性能损耗是非常明显的；二是如果任务处于不同的taskmanager，那数据传输还涉及到网络传输。另外合并算子也减少了整个job的线程数，能够减少线程转化的开销。</p><p>需要注意的是，合并算子并不一定能带来性能提升的，因为算子合并其实相当于减少了并发，可能会影响CPU利用率，可以参考多线程的线程数考虑这一点。</p><h2 id="执行图（ExecutionGraph"><a href="#执行图（ExecutionGraph" class="headerlink" title="执行图（ExecutionGraph)"></a>执行图（ExecutionGraph)</h2><p>相关概念介绍完后，简单介绍一下（很多细节还未搞明白，但尚不影响使用）一个Flink作业是如何一步步转化为Taskmanager上可以执行的task的。下面的描述主要针对Session部署方式，对于Application部署模式之后再介绍。</p><p>首先，客户端会将代码转化为dataflow，dataflow进一步优化如合并算子链后生成JobGraph。<br><img src="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/2022-12-21-22-37-05.png"></p><p>然后，JM对JobGraph根据并行度进行拆分生成执行图，<br><img src="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/2022-12-21-22-38-45.png"></p><p>最后JM会分发执行图到taskmanager上，实际执行的叫物理执行图。</p><h1 id="Flink的资源分配和调度"><a href="#Flink的资源分配和调度" class="headerlink" title="Flink的资源分配和调度"></a>Flink的资源分配和调度</h1><p>slots是Flink中资源分配的最小单位。Flink对内存资源是进行了隔离的，隔离出来的每一份资源叫一个slot。每个TM通过参数taskmanager.numberOfTaskSlots配置slots的数量。建议根据核的数量分配任务槽，这样一个任务槽就一个cpu核，cpu就不需要分时复用了。默认slots平分整个TM的内存资源，Flink也支持细粒度地划分slots的资源。<br><img src="/2022-12-22-09-52-13.png"></p><p>需要配置cluster.fine-grained-resource-management.enabled为true</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-type">StreamExecutionEnvironment</span> <span class="hljs-variable">env</span> <span class="hljs-operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();<br><br><span class="hljs-type">SlotSharingGroup</span> <span class="hljs-variable">ssgA</span> <span class="hljs-operator">=</span> SlotSharingGroup.newBuilder(<span class="hljs-string">&quot;a&quot;</span>)<br>  .setCpuCores(<span class="hljs-number">1.0</span>)<br>  .setTaskHeapMemoryMB(<span class="hljs-number">100</span>)<br>  .build();<br><br><span class="hljs-type">SlotSharingGroup</span> <span class="hljs-variable">ssgB</span> <span class="hljs-operator">=</span> SlotSharingGroup.newBuilder(<span class="hljs-string">&quot;b&quot;</span>)<br>  .setCpuCores(<span class="hljs-number">0.5</span>)<br>  .setTaskHeapMemoryMB(<span class="hljs-number">100</span>)<br>  .build();<br><br>someStream.filter(...).slotSharingGroup(<span class="hljs-string">&quot;a&quot;</span>) <span class="hljs-comment">// Set the slot sharing group with name “a”</span><br>.map(...).slotSharingGroup(ssgB); <span class="hljs-comment">// Directly set the slot sharing group with name and resource.</span><br><br>env.registerSlotSharingGroup(ssgA); <span class="hljs-comment">// Then register the resource of group “a”</span><br></code></pre></td></tr></table></figure><p><img src="/2022/12/21/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAFlink%E9%9B%86%E7%BE%A4/2022-12-21-22-54-12.png"></p><p>上面讲的是资源的分配，再讲资源的调度：不同的task如何分配到slots上面。 主要遵守下面两个原则： 同一个任务的不同子任务只能分配到不同的slots上；多个任务可以共享slot。以上图为例，一共3个算子链，并行度分别为6，6，1，每个算子链在slots上依次分配，同一个Job的不同算子链共享slot的。</p><p>基于这样的资源调度规则，就不难理解“一个job需要的任务槽的数量至少为算子链的最大并行度“。像上面的示例，需要的任务槽数量就是6。</p><p>为什么slots可以共享？不同的task资源完全隔离不好吗？这里主要是从提高资源的利用率考虑的，希望各个内存区域的使用相对均衡，而不是忙的忙死闲的闲死。</p><h1 id="Flink的部署"><a href="#Flink的部署" class="headerlink" title="Flink的部署"></a>Flink的部署</h1><p>Flink提供了3种部署模式：</p><ul><li>会话模式（Session Mode)</li><li>单作业模式（Per-Job Mode)</li><li>应用模式（Application Mode)</li></ul><p>它们的区别主要在于：集群的生命周期以及资源的分配方式；应用的Main方法在哪里执行——客户端还是JobManager。 其中Per-Job模式在1.15版本后已经废弃，就不再介绍了。</p><h2 id="会话模式"><a href="#会话模式" class="headerlink" title="会话模式"></a>会话模式</h2><p>先启动集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源就都已经确定，所以所有提交的作业会竞争集群中的资源。一个任务导致集群崩溃会牵连其他所有任务。</p><p>会话模式适合单个规模小、执行时间短的大量作业。（因为执行时间短，所以单个作业占用的资源很快能释放掉给下一个作业使用，不需要反复启动集群，反复部署资源）</p><h2 id="应用模式"><a href="#应用模式" class="headerlink" title="应用模式"></a>应用模式</h2><p>应用模式是提交任务的同时启动集群，一个应用一个集群，应用在集群在，应用亡集群自动关闭。此外，应用模式的另一个显著特点是应用的main方法执行在JM，而不是客户端。这样做是为了减轻客户端的负载，避免当多个用户同时提交任务时客户端宕机。</p><p>那么，main方法的执行为什么会带来较大的负载呢？执行main方法首先需要下载相关的依赖，还需要抽取拓扑结构（比如JobGraph）便于后续的处理。客户端执行完后还需要把这些都传输给JM。这就使得客户端一是需要格外的网络带宽下载依赖，传输数据给JM; 二是消耗更多的CPU。因此application模式把这部分的工作放在了JM上。</p><p>官方推荐在产线上使用应用模式，在测试开发中使用会话模式。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://flink.apache.org/">official document</a></li><li><a href="https://www.bilibili.com/video/BV133411s7Sa/?vd_source=4ae95225239e82c38fe3a820e863882e">B站视频</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Flink</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
      <tag>Cluster Deployment</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从0开始部署一个Flink集群：实践篇（Native k8s部署）</title>
    <link href="/2022/12/19/Flink%E9%83%A8%E7%BD%B2k8s%E7%AF%87/"/>
    <url>/2022/12/19/Flink%E9%83%A8%E7%BD%B2k8s%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<p>上一篇博文介绍了如何独立部署一个高可用的Flink集群,本篇介绍如何用Native k8s去部署高可用的Flink 集群。<br>EC2操作系统：centos7<br>本机操作系统：Mac<br>flink version: 1.14<br>jdk version: java11<br>HA service: k8s<br>File System: S3</p><h1 id="启动EC2"><a href="#启动EC2" class="headerlink" title="启动EC2"></a>启动EC2</h1><p>在AWS上启动3个EC2,操作系统选centos7,注意每个EC2要关联弹性ip地址，允许外网访问。</p><h1 id="部署k8s集群"><a href="#部署k8s集群" class="headerlink" title="部署k8s集群"></a>部署k8s集群</h1><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo yum-config-manager --add-repo https:<span class="hljs-regexp">//</span>download.docker.com<span class="hljs-regexp">/linux/</span>centos/docker-ce.repo<br>sudo yum install docker -y<br>sudo systemctl start docker.service<br>sudo systemctl enable docker.service<br>sudo systemctl status docker<br></code></pre></td></tr></table></figure><h2 id="安装k8s"><a href="#安装k8s" class="headerlink" title="安装k8s"></a>安装k8s</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ sudo vi <span class="hljs-regexp">/etc/yum</span>.repos.d/kubernetes.repo<br>[kubernetes]<br>name=Kubernetes<br>baseurl=https:<span class="hljs-regexp">//</span>packages.cloud.google.com<span class="hljs-regexp">/yum/</span>repos/kubernetes-el7-x86_64<br>enabled=<span class="hljs-number">1</span><br>gpgcheck=<span class="hljs-number">1</span><br>repo_gpgcheck=<span class="hljs-number">0</span><br>gpgkey=https:<span class="hljs-regexp">//</span>packages.cloud.google.com<span class="hljs-regexp">/yum/</span>doc<span class="hljs-regexp">/yum-key.gpg https:/</span><span class="hljs-regexp">/packages.cloud.google.com/yum</span><span class="hljs-regexp">/doc/</span>rpm-package-key.gpg<br>$ sudo yum install -y kubelet<br>$ sudo yum install -y kubeadm<br>$ sudo systemctl enable kubelet<br></code></pre></td></tr></table></figure><h2 id="Linux环境准备"><a href="#Linux环境准备" class="headerlink" title="Linux环境准备"></a>Linux环境准备</h2><ol><li>set hostnames</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// On your **Master** node, update your hostname using the following command:</span><br><br><span class="hljs-comment">//Master:192.168.100.90 (3.101.77.138)</span><br><br>$ sudo hostnamectl set-hostname master-node<br><br><span class="hljs-comment">//worker-node1:192.168.100.29 (3.101.77.139)</span><br><br>$ sudo hostnamectl set-hostname worker-node1<br><br><span class="hljs-comment">// worker-node2: 192.168.100.21 (3.101.77.140)</span><br><br>$ sudo hostnamectl set-hostname worker-node2<br><br><span class="hljs-comment">// Make a host entry or DNS record to resolve the hostname for all nodes:</span><br><br>$ sudo vi /etc/hosts<br><br><span class="hljs-comment">// With the entry:</span><br><br><span class="hljs-number">192.168</span><span class="hljs-number">.100</span><span class="hljs-number">.63</span>  master-node<br><br><span class="hljs-number">192.168</span><span class="hljs-number">.100</span><span class="hljs-number">.36</span> node1 worker-node1<br><br><span class="hljs-number">192.168</span><span class="hljs-number">.100</span><span class="hljs-number">.125</span> node2 worker-node2<br></code></pre></td></tr></table></figure><ol start="2"><li>关闭SElinux<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">$ sudo setenforce <span class="hljs-number">0</span><br>$ sudo sed -i --follow-symlinks <span class="hljs-string">&#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27;</span> /etc/sysconfig/selinux<br>$ sudo reboot<br>$ sestatus<br>SELinux status:                 disabled<br></code></pre></td></tr></table></figure></li><li>update iptables config</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java">$ sudo vi /etc/sysctl.d/k8s.conf<br>net.bridge.bridge-nf-call-ip6tables = <span class="hljs-number">1</span><br>net.bridge.bridge-nf-call-iptables = <span class="hljs-number">1</span><br>$ sudo sysctl --system<br></code></pre></td></tr></table></figure><ol start="4"><li>disable swap(3)</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">$ sudo sed -i <span class="hljs-string">&#x27;/swap/d&#x27;</span> /etc/fstab<br>$ sudo swapoff -a<br></code></pre></td></tr></table></figure><h2 id="部署k8s集群-1"><a href="#部署k8s集群-1" class="headerlink" title="部署k8s集群"></a>部署k8s集群</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// only master node</span><br>$ sudo kubeadm init<br>kubeadm join <span class="hljs-number">192.168</span><span class="hljs-number">.100</span><span class="hljs-number">.63</span>:<span class="hljs-number">6443</span> --token snmpyy.b4y506h6hr9u7fxh \<br>        --discovery-token-ca-cert-hash sha256:87a099765ce369c519bc02af84a6d4732b1cb987d3e95277b334e3cfc3aa0960<br><span class="hljs-comment">// Create required directories and start managing Kubernetes cluster</span><br>$ mkdir -p $HOME/.kube<br>$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>$ sudo chown $(id -u):$(id -g) $HOME/.kube/config<br>[ec2-user<span class="hljs-meta">@master</span>-node kubernetes]$ kubectl get nodes<br>NAME          STATUS     ROLES           AGE   VERSION<br>master-node   NotReady   control-plane   84m   v1<span class="hljs-number">.24</span><span class="hljs-number">.2</span><br><span class="hljs-comment">//Set up Pod network for the Cluster</span><br>$ export kubever=$(kubectl version | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>)<br>$ kubectl apply -f https:<span class="hljs-comment">//cloud.weave.works/k8s/net?k8s-version=$kubever</span><br><span class="hljs-comment">// add nodes to your cluster</span><br><span class="hljs-comment">//在两个工作节点操作</span><br>$ kubeadm join <span class="hljs-number">192.168</span><span class="hljs-number">.100</span><span class="hljs-number">.63</span>:<span class="hljs-number">6443</span> --token snmpyy.b4y506h6hr9u7fxh \<br>        --discovery-token-ca-cert-hash sha256:87a099765ce369c519bc02af84a6d4732b1cb987d3e95277b334e3cfc3aa0960<br><span class="hljs-comment">//在master节点</span><br>$ kubectl label node worker-node1 node-role.kubernetes.io/worker=worker<br>$ kubectl label node worker-node2 node-role.kubernetes.io/worker=worker<br><br></code></pre></td></tr></table></figure><h1 id="Session-Mode-部署Flink集群"><a href="#Session-Mode-部署Flink集群" class="headerlink" title="Session Mode 部署Flink集群"></a>Session Mode 部署Flink集群</h1><h2 id="下载解压Flink安装包"><a href="#下载解压Flink安装包" class="headerlink" title="下载解压Flink安装包"></a>下载解压Flink安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> /opt</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">wget https://dlcdn.apache.org/flink/flink-1.15.2/flink-1.15.2-bin-scala_2.12.tgz --no-check-certificate</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">tar -xzf flink-*.tgz</span><br></code></pre></td></tr></table></figure><h2 id="启动集群（无高可用服务版本）"><a href="#启动集群（无高可用服务版本）" class="headerlink" title="启动集群（无高可用服务版本）"></a>启动集群（无高可用服务版本）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//service account with RBAC permissions to create, delete pods</span><br>$ kubectl create namespace flink-cluster<br>$ kubectl create serviceaccount flink -n flink-cluster<br>$ kubectl create clusterrolebinding flink-role-binding-flink \<br>  --clusterrole=edit \<br>  --serviceaccount=flink-cluster:flink<br><span class="hljs-comment">//启动session集群1（此时读取的配置为master节点的配置文件，与taskmanager节点的配置文件无关）</span><br>$ ./bin/kubernetes-session.sh -Dkubernetes.namespace=flink-cluster -Dkubernetes.jobmanager.service-account=flink -Dkubernetes.cluster-id=my-session -Dtaskmanager.numberOfTaskSlots=<span class="hljs-number">6</span>  -Dkubernetes.rest-service.exposed.type=NodePort<br><span class="hljs-number">2022</span>-<span class="hljs-number">07</span>-<span class="hljs-number">01</span> 08:<span class="hljs-number">46</span>:<span class="hljs-number">49</span>,<span class="hljs-number">621</span> INFO  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create flink session cluster my-session successfully, JobManager Web Interface: http:<span class="hljs-comment">//192.168.100.63:32172</span><br><span class="hljs-comment">// Dashboard: http://3.101.77.141:32172/,此时没有任何资源\</span><br><br><span class="hljs-comment">//只部署了jobmanager，部署在worker-node1,两个服务，一个对内，一个对外</span><br>[root<span class="hljs-meta">@master</span>-node ec2-user]# kubectl get pods,svc,ep -n flink-cluster -o wide<br>NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   <span class="hljs-title function_">PORT</span><span class="hljs-params">(S)</span>             AGE    SELECTOR<br>service/my-session        ClusterIP   None            &lt;none&gt;        <span class="hljs-number">6123</span>/TCP,<span class="hljs-number">6124</span>/TCP   124m   app=my-session,component=jobmanager,type=flink-<span class="hljs-keyword">native</span>-kubernetes<br>service/my-session-rest   NodePort    <span class="hljs-number">10.109</span><span class="hljs-number">.225</span><span class="hljs-number">.42</span>   &lt;none&gt;        <span class="hljs-number">8081</span>:<span class="hljs-number">31595</span>/TCP      124m   app=my-session,component=jobmanager,type=flink-<span class="hljs-keyword">native</span>-kubernetes<br><br>NAME                              READY   STATUS    RESTARTS   AGE    IP          NODE           NOMINATED NODE   READINESS GATES<br>pod/my-session-556f44f44b-94gfk   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          124m   <span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>   worker-node1   &lt;none&gt;           &lt;none&gt;<br><br>NAME                        ENDPOINTS                       AGE<br>endpoints/my-session        <span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>:<span class="hljs-number">6124</span>,<span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>:<span class="hljs-number">6123</span>   124m<br>endpoints/my-session-rest   <span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>:<span class="hljs-number">8081</span>                  124m<br>[root<span class="hljs-meta">@master</span>-node ec2-user]# kubectl get deployment -o wide -n flink-cluster<br>NAME         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS             IMAGES                           SELECTOR<br>my-session   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           125m   flink-main-container   apache/flink:<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>-scala_2<span class="hljs-number">.12</span>   app=my-session,component=jobmanager,type=flink-<span class="hljs-keyword">native</span>-kubernetes<br><br></code></pre></td></tr></table></figure><p>k8s默认pod不可以调度到master节点，所以flink的master实际是部署在k8s的workernode上。<br>可以通过node-selector指定jobmanager的节点</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//启动Session cluster2, 修改配置</span><br>$ kubectl label nodes worker-node2 node=master<br>[root<span class="hljs-meta">@master</span>-node flink-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>]# kubectl get nodes -l <span class="hljs-string">&quot;node=master&quot;</span><br>NAME           STATUS   ROLES    AGE   VERSION<br>worker-node2   Ready    worker   <span class="hljs-number">10d</span>   v1<span class="hljs-number">.24</span><span class="hljs-number">.2</span><br>$ ./bin/kubernetes-session.sh -Dkubernetes.namespace=flink-cluster -Dkubernetes.jobmanager.service-account=flink -Dkubernetes.cluster-id=my-session -Dtaskmanager.numberOfTaskSlots=<span class="hljs-number">8</span>  -D<br>kubernetes.rest-service.exposed.type=NodePort -Dkubernetes.jobmanager.node-selector=node:master<br>[root<span class="hljs-meta">@master</span>-node flink-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>]# kubectl get pods,svc,ep -n flink-cluster -o wide<br>NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE           NOMINATED NODE   READINESS GATES<br>pod/my-session-58bb97cdc-85ssq   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          57s   <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>   worker-node2   &lt;none&gt;           &lt;none&gt;<br><br>NAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   <span class="hljs-title function_">PORT</span><span class="hljs-params">(S)</span>             AGE   SELECTOR<br>service/my-session        ClusterIP   None          &lt;none&gt;        <span class="hljs-number">6123</span>/TCP,<span class="hljs-number">6124</span>/TCP   57s   app=my-session,component=jobmanager,type=flink-<span class="hljs-keyword">native</span>-kubernetes<br>service/my-session-rest   NodePort    <span class="hljs-number">10.108</span><span class="hljs-number">.58</span><span class="hljs-number">.5</span>   &lt;none&gt;        <span class="hljs-number">8081</span>:<span class="hljs-number">31181</span>/TCP      57s   app=my-session,component=jobmanager,type=flink-<span class="hljs-keyword">native</span>-kubernetes<br><br>NAME                        ENDPOINTS                       AGE<br>endpoints/my-session        <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6124</span>,<span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6123</span>   57s<br>endpoints/my-session-rest   <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">8081</span>                  57s<br></code></pre></td></tr></table></figure><h2 id="启动集群（含高可用服务版本）"><a href="#启动集群（含高可用服务版本）" class="headerlink" title="启动集群（含高可用服务版本）"></a>启动集群（含高可用服务版本）</h2><p>高可用服务的部署需要一个可共享的持久化存储目录，这里我选择S3。<br>因此首先要解决的问题是如何让集群可以使用S3</p><h3 id="配置s3"><a href="#配置s3" class="headerlink" title="配置s3"></a>配置s3</h3><ol><li>首先要赋予EC2 访问S3 bucket的权限，这可以通过添加IAM实现。</li><li>Copy the respective JAR file（flink-s3-fs-hadoop-1.14.4.jar） from the opt directory to the plugins directory of your Flink distribution before starting Flink. 直接cp即可</li><li>启动集群的时候enable内置的插件就可以使用s3了<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">./bin/kubernetes-session<span class="hljs-selector-class">.sh</span><br>    -Dcontainerized<span class="hljs-selector-class">.master</span><span class="hljs-selector-class">.env</span>.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-<span class="hljs-number">1.14</span>.<span class="hljs-number">4</span><span class="hljs-selector-class">.jar</span> \<br>    -Dcontainerized<span class="hljs-selector-class">.taskmanager</span><span class="hljs-selector-class">.env</span>.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-<span class="hljs-number">1.14</span>.<span class="hljs-number">4</span>.jar<br></code></pre></td></tr></table></figure></li></ol><h3 id="启动高可用集群"><a href="#启动高可用集群" class="headerlink" title="启动高可用集群"></a>启动高可用集群</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">./bin/kubernetes-session.sh -Dkubernetes.namespace=flink-cluster -Dkubernetes.jobmanager.service-account=flink -Dkubernetes.cluster-id=my-session -Dtaskmanager.numberOfTaskSlots=<span class="hljs-number">6</span>  -Dkubernetes.rest-service.exposed.type=NodePort -Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>.jar -Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>.jar -Dhigh-availability=org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory -Dhigh-availability.storageDir=s3a:<span class="hljs-comment">//yunzpeng-bucket/flink-ha </span><br></code></pre></td></tr></table></figure><h2 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h2><p>可以通过命令行提交，也可以通过dashboard提交任务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// run之后才会显示资源</span><br>$ ./bin/flink run \<br>    --target kubernetes-session \<br>-Dkubernetes.namespace=flink-cluster \<br>    -Dkubernetes.cluster-id=my-session \<br>    ./examples/streaming/TopSpeedWindowing.jar<br>$ ./bin/flink run     --target kubernetes-session     -Dkubernetes.namespace=flink-cluster     -Dkubernetes.cluster-id=my-session   -Dparallelism.<span class="hljs-keyword">default</span>=<span class="hljs-number">2</span>  ./examples/streaming/TopSpeedWindowing.jar<br><span class="hljs-comment">//自动扩容功能测试</span><br><span class="hljs-comment">//一个taskmanager的任务槽使用完之前，cluster只有一个task manager</span><br>NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE           NOMINATED NODE   READINESS GATES<br>my-session-556f44f44b-zdvrf   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          21m   <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>   worker-node2   &lt;none&gt;           &lt;none&gt;<br>my-session-taskmanager-<span class="hljs-number">1</span>-<span class="hljs-number">1</span>    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          20m   <span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>   worker-node1   &lt;none&gt;           &lt;none&gt;<br><span class="hljs-comment">//继续提交作业，cluster会自动扩容</span><br>NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE           NOMINATED NODE   READINESS GATES<br>my-session-556f44f44b-zdvrf   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          21m   <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>   worker-node2   &lt;none&gt;           &lt;none&gt;<br>my-session-taskmanager-<span class="hljs-number">1</span>-<span class="hljs-number">1</span>    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          20m   <span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>   worker-node1   &lt;none&gt;           &lt;none&gt;<br>my-session-taskmanager-<span class="hljs-number">1</span>-<span class="hljs-number">2</span>    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          34s   <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>   worker-node2   &lt;none&gt;           &lt;none&gt;<br><span class="hljs-comment">//当两个taskmanager的资源都用完了</span><br>[root<span class="hljs-meta">@master</span>-node flink-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>]kubectl get pods -n flink-cluster -o wide<br>NAME                         READY   STATUS    RESTARTS   AGE     IP          NODE           NOMINATED NODE   READINESS GATES<br>my-session-58bb97cdc-85ssq   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          17m     <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>   worker-node2   &lt;none&gt;           &lt;none&gt;<br>my-session-taskmanager-<span class="hljs-number">1</span>-<span class="hljs-number">1</span>   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          8m26s   <span class="hljs-number">10.44</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>   worker-node1   &lt;none&gt;           &lt;none&gt;<br>my-session-taskmanager-<span class="hljs-number">1</span>-<span class="hljs-number">2</span>   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          3m11s   <span class="hljs-number">10.36</span><span class="hljs-number">.0</span><span class="hljs-number">.3</span>   worker-node2   &lt;none&gt;           &lt;none&gt;<br>my-session-taskmanager-<span class="hljs-number">1</span>-<span class="hljs-number">3</span>   <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     Pending   <span class="hljs-number">0</span>          33s     &lt;none&gt;      &lt;none&gt;         &lt;none&gt;           &lt;none&gt;<br><span class="hljs-comment">//清理资源</span><br>$ kubectl delete deployment/my-session -n flink-cluster<br>$ kubectl delete clusterrolebinding flink-role-binding-flink<br>$ kubectl delete serviceaccount flink -n flink-cluster<br>$ kubectl delete namespace flink-cluster<br></code></pre></td></tr></table></figure><h2 id="总结：Native-k8s部署下session模式集群启动和作业提交过程"><a href="#总结：Native-k8s部署下session模式集群启动和作业提交过程" class="headerlink" title="总结：Native k8s部署下session模式集群启动和作业提交过程"></a>总结：Native k8s部署下session模式集群启动和作业提交过程</h2><p><img src="/2022/12/19/Flink%E9%83%A8%E7%BD%B2k8s%E7%AF%87/2022-12-22-00-00-17.png"></p><ul><li>第一个阶段：启动 Session Cluster。Flink Client 内置了 K8s Client，告诉 K8s Master 创建 Flink Master Deployment，ConfigMap，SVC。创建完成后，Master 就拉起来了。这时，Session 就部署完成了，并没有维护任何 TaskManager。</li><li>第二个阶段：当用户提交 Job 时，可以通过 Flink Client 或者 Dashboard 的方式，然后通过 Service 到 Dispatcher，Dispatcher 会产生一个 JobMaster。JobMaster 会向 K8sResourceManager 申请资源。ResourceManager 会发现现在没有任何可用的资源，它就会继续向 K8s 的 Master 去请求资源，请求资源之后将其发送回去，起新的 Taskmanager。Taskmanager 起来之后，再注册回来，此时的 ResourceManager 再向它去申请 slot 提供给 JobMaster，最后由 JobMaster 将相应的 Task 部署到 TaskManager 上。这样整个从 Session 的拉起到用户提交都完成了。</li></ul><h1 id="Applicatiob-Mode部署Flink-集群"><a href="#Applicatiob-Mode部署Flink-集群" class="headerlink" title="Applicatiob Mode部署Flink 集群"></a>Applicatiob Mode部署Flink 集群</h1><h2 id="无高可用"><a href="#无高可用" class="headerlink" title="无高可用"></a>无高可用</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 在root下操作</span><br><span class="hljs-comment">// 1. build a docker image with the flink job</span><br>FROM flink:<span class="hljs-number">1.14</span><span class="hljs-number">.5</span><br>RUN mkdir -p $FLINK_HOME/usrlib<br>COPY ./examples/streaming/TopSpeedWindowing.jar $FLINK_HOME/usrlib/my-flink-job.jar<br>$ docker build -t pandafish1996/flink-demo .<br><span class="hljs-comment">// 2. Push image to image warehouse</span><br>$ docker login --username=pandafish1996<br><span class="hljs-comment">// 规范： docker push 注册用户名/镜像名</span><br><span class="hljs-comment">//$ docker tag yunzpeng/flink-word-count pandafish1996/flink-word-count</span><br>$ docker push pandafish1996/flink-demo <br><br><span class="hljs-comment">//3. start a flink application cluster</span><br>$ kubectl create namespace flink-cluster<br>$ kubectl create serviceaccount flink -n flink-cluster<br>$ kubectl create clusterrolebinding flink-role-binding-flink \<br>  --clusterrole=edit \<br>  --serviceaccount=flink-cluster:flink<br><span class="hljs-comment">// without HA</span><br>$ ./bin/flink run-application \<br>--target kubernetes-application \<br>-Dkubernetes.cluster-id=my-first-application-cluster \<br>-Dkubernetes.container.image=pandafish1996/flink-demo \<br>-Dkubernetes.namespace=flink-cluster \<br>-Dkubernetes.service-account=flink \<br>-Dparallelism.<span class="hljs-keyword">default</span>=<span class="hljs-number">2</span> \<br>-Dtaskmanager.numberOfTaskSlots=<span class="hljs-number">6</span> \<br>-Dkubernetes.rest-service.exposed.type=NodePort  \<br>local:<span class="hljs-comment">///opt/flink/usrlib/my-flink-job.jar</span><br><br>$ kubectl delete deployment/my-first-application-cluster -n flink-cluster<br></code></pre></td></tr></table></figure><h2 id="高可用版本"><a href="#高可用版本" class="headerlink" title="高可用版本"></a>高可用版本</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 1. 构造image并上传到远程仓库</span><br>FROM flink:<span class="hljs-number">1.14</span><span class="hljs-number">.5</span><br>RUN mkdir -p $FLINK_HOME/usrlib<br>COPY ./examples/streaming/TopSpeedWindowing.jar $FLINK_HOME/usrlib/my-flink-job.jar<br>RUN mkdir -p $FLINK_HOME/plugins/flink-s3-fs-hadoop<br>COPY ./opt/flink-s3-fs-hadoop-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>.jar $FLINK_HOME/plugins/flink-s3-fs-hadoop/<br>$ docker build -t pandafish1996/flink-hademo .<br>$ docker login --username=pandafish1996<br><span class="hljs-comment">// 规范： docker push 注册用户名/镜像名</span><br><span class="hljs-comment">//$ docker tag yunzpeng/flink-word-count pandafish1996/flink-word-count</span><br>$ docker push pandafish1996/flink-hademo <br><span class="hljs-comment">// 2. 启动集群</span><br>./bin/flink run-application \<br>--target kubernetes-application \<br>-Dkubernetes.cluster-id=ha-cluster1 \<br>-Dkubernetes.container.image=pandafish1996/flink-hademo \<br>-Dkubernetes.namespace=flink-cluster \<br>-Dkubernetes.service-account=flink \<br>-Dparallelism.<span class="hljs-keyword">default</span>=<span class="hljs-number">2</span> \<br>-Dtaskmanager.numberOfTaskSlots=<span class="hljs-number">6</span> \<br>-Dkubernetes.rest-service.exposed.type=NodePort  \<br>-Dhigh-availability=org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory \<br>-Dhigh-availability.storageDir=s3:<span class="hljs-comment">//yunzpeng-bucket/flink-ha \</span><br>-Drestart-strategy=fixed-delay \<br>-Drestart-strategy.fixed-delay.attempts=<span class="hljs-number">10</span> \<br>-Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>.jar \<br>-Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-<span class="hljs-number">1.14</span><span class="hljs-number">.5</span>.jar \<br>local:<span class="hljs-comment">///opt/flink/usrlib/my-flink-job.jar</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Flink</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
      <tag>Cluster Deployment</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ssh免密登录遇到过的坑</title>
    <link href="/2022/12/19/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%AF%A6%E8%A7%A3/"/>
    <url>/2022/12/19/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="ssh免密登录原理"><a href="#ssh免密登录原理" class="headerlink" title="ssh免密登录原理"></a>ssh免密登录原理</h1><p>通过非对称密钥实现认证登录，可参考<br><a href="https://cloud.tencent.com/developer/article/1484468">ssh免密登录原理与实现</a><br><a href="https://zhuanlan.zhihu.com/p/241341815">SSH 原理和基本应用</a></p><ol><li>客户端和服务器都生成自己的密钥对</li><li>客户端将公钥写入服务器的authorized_keys</li><li>ssh server远程访问服务器，发送连接请求，并发送id_rsa.pub公钥，服务器在本地的authorized_keys中查找是否存在该公钥，如果存在，用该公钥对任意字符串加密发送回客户端，客户端使用本地的id_rsa解密发送回服务器，服务器验证两个字符串是否相同。</li></ol><h1 id="ssh免密的登录设置方法"><a href="#ssh免密的登录设置方法" class="headerlink" title="ssh免密的登录设置方法"></a>ssh免密的登录设置方法</h1><ol><li>进入当前账号的home目录，进入.ssh文件夹</li><li>ssh-keygen 生成密钥对，输入命令后一直回车即可</li><li>复制公钥到远程服务器的.ssh目录下的authorized_keys文件中，有三种方法：<ol><li>ssh-copy-id命令</li><li>scp命令</li><li>手工复制粘贴</li></ol></li></ol><h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><h2 id="非root账号"><a href="#非root账号" class="headerlink" title="非root账号"></a>非root账号</h2><ol><li>非root账号下设置ssh免密登录，使用ssh-copy-id命令会要求输入远程服务器的root密码，可通过手工复制粘贴解决。</li></ol><h2 id="root账号"><a href="#root账号" class="headerlink" title="root账号"></a>root账号</h2><p>root账号下设置ssh免密登录后，始终要求输入root密码。<br>解决思路：</p><ol><li>检查本地生成密钥的.ssh文件夹和远程服务器上公钥复制粘贴操作的.ssh文件夹是否都是&#x2F;root目录下的。我刚开始是把登录账号的.ssh目录和root账号的.ssh目录混在了一起。注意，在某账号下ssh server1相当于ssh 账号名@server1，不同账号的远程登录目录不一样，检索的位置就不一样。</li><li>检查etc&#x2F;ssh&#x2F;sshd_config配置，DenyUsers root, DenyGroups root这两行代表禁止通过远程访问根用户，需要注释掉。其它的配置也需要检查。这里我粘贴一下我最终的配置。<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">#$OpenBSD: sshd_config,v 1.100 2016/08/15 12:32:04 naddy Exp $</span><br><br><span class="hljs-comment"># This is the sshd server system-wide configuration file.  See</span><br><span class="hljs-comment"># sshd_config(5) for more information.</span><br><br><span class="hljs-comment"># This sshd was compiled with PATH=/usr/local/bin:/usr/bin</span><br><br><span class="hljs-comment"># The strategy used for options in the default sshd_config shipped with</span><br><span class="hljs-comment"># OpenSSH is to specify options with their default value where</span><br><span class="hljs-comment"># possible, but leave them commented.  Uncommented options override the</span><br><span class="hljs-comment"># default value.</span><br><br><span class="hljs-comment"># If you want to change the port on a SELinux system, you have to tell</span><br><span class="hljs-comment"># SELinux about this change.</span><br><span class="hljs-comment"># semanage port -a -t ssh_port_t -p tcp #PORTNUMBER</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#Port 22</span><br><span class="hljs-comment">#AddressFamily any</span><br><span class="hljs-comment">#ListenAddress 0.0.0.0</span><br><span class="hljs-comment">#ListenAddress ::</span><br><br>HostKey /etc/ssh/ssh_host_rsa_key<br><span class="hljs-comment">#HostKey /etc/ssh/ssh_host_dsa_key</span><br>HostKey /etc/ssh/ssh_host_ecdsa_key<br>HostKey /etc/ssh/ssh_host_ed25519_key<br><br><span class="hljs-comment"># Ciphers and keying</span><br><span class="hljs-comment">#RekeyLimit default none</span><br><br><span class="hljs-comment"># Logging</span><br><span class="hljs-comment">#SyslogFacility AUTH</span><br>SyslogFacility AUTHPRIV<br>LogLevel <span class="hljs-built_in">INFO</span><br><br><span class="hljs-comment"># Authentication:</span><br><br>LoginGraceTime 60<br>PermitRootLogin <span class="hljs-literal">yes</span><br><span class="hljs-comment">#StrictModes yes</span><br>MaxAuthTries 4<br><span class="hljs-comment">#MaxSessions 10</span><br><br>PubkeyAuthentication <span class="hljs-literal">yes</span><br><br><span class="hljs-comment"># The default is to check both .ssh/authorized_keys and .ssh/authorized_keys2</span><br><span class="hljs-comment"># but this is overridden so installations will only check .ssh/authorized_keys</span><br>AuthorizedKeysFile.ssh/authorized_keys<br><br><span class="hljs-comment">#AuthorizedPrincipalsFile none</span><br><br><span class="hljs-comment">#AuthorizedKeysCommand none</span><br><span class="hljs-comment">#AuthorizedKeysCommandUser nobody</span><br><br><span class="hljs-comment"># For this to work you will also need host keys in /etc/ssh/ssh_known_hosts</span><br>HostbasedAuthentication <span class="hljs-literal">no</span><br><span class="hljs-comment"># Change to yes if you don&#x27;t trust ~/.ssh/known_hosts for</span><br><span class="hljs-comment"># HostbasedAuthentication</span><br><span class="hljs-comment">#IgnoreUserKnownHosts no</span><br><span class="hljs-comment"># Don&#x27;t read the user&#x27;s ~/.rhosts and ~/.shosts files</span><br>IgnoreRhosts <span class="hljs-literal">yes</span><br><br><span class="hljs-comment"># To disable tunneled clear text passwords, change to no here!</span><br><span class="hljs-comment">#PasswordAuthentication yes</span><br>PermitEmptyPasswords <span class="hljs-literal">no</span><br>PasswordAuthentication <span class="hljs-literal">yes</span><br><br><span class="hljs-comment"># Change to no to disable s/key passwords</span><br><span class="hljs-comment">#ChallengeResponseAuthentication yes</span><br>ChallengeResponseAuthentication <span class="hljs-literal">yes</span><br><br><span class="hljs-comment"># Kerberos options</span><br><span class="hljs-comment">#KerberosAuthentication no</span><br><span class="hljs-comment">#KerberosOrLocalPasswd yes</span><br><span class="hljs-comment">#KerberosTicketCleanup yes</span><br><span class="hljs-comment">#KerberosGetAFSToken no</span><br><span class="hljs-comment">#KerberosUseKuserok yes</span><br><br><span class="hljs-comment"># GSSAPI options</span><br>GSSAPIAuthentication <span class="hljs-literal">yes</span><br>GSSAPICleanupCredentials <span class="hljs-literal">no</span><br><span class="hljs-comment">#GSSAPIStrictAcceptorCheck yes</span><br><span class="hljs-comment">#GSSAPIKeyExchange no</span><br><span class="hljs-comment">#GSSAPIEnablek5users no</span><br><br><span class="hljs-comment"># Set this to &#x27;yes&#x27; to enable PAM authentication, account processing,</span><br><span class="hljs-comment"># and session processing. If this is enabled, PAM authentication will</span><br><span class="hljs-comment"># be allowed through the ChallengeResponseAuthentication and</span><br><span class="hljs-comment"># PasswordAuthentication.  Depending on your PAM configuration,</span><br><span class="hljs-comment"># PAM authentication via ChallengeResponseAuthentication may bypass</span><br><span class="hljs-comment"># the setting of &quot;PermitRootLogin without-password&quot;.</span><br><span class="hljs-comment"># If you just want the PAM account and session checks to run without</span><br><span class="hljs-comment"># PAM authentication, then enable this but set PasswordAuthentication</span><br><span class="hljs-comment"># and ChallengeResponseAuthentication to &#x27;no&#x27;.</span><br><span class="hljs-comment"># WARNING: &#x27;UsePAM no&#x27; is not supported in Red Hat Enterprise Linux and may cause several</span><br><span class="hljs-comment"># problems.</span><br>UsePAM <span class="hljs-literal">yes</span><br><br><span class="hljs-comment">#AllowAgentForwarding yes</span><br><span class="hljs-comment">#AllowTcpForwarding yes</span><br><span class="hljs-comment">#GatewayPorts no</span><br>X11Forwarding <span class="hljs-literal">no</span><br><span class="hljs-comment">#X11DisplayOffset 10</span><br><span class="hljs-comment">#X11UseLocalhost yes</span><br><span class="hljs-comment">#PermitTTY yes</span><br><span class="hljs-comment">#PrintMotd yes</span><br><span class="hljs-comment">#PrintLastLog yes</span><br><span class="hljs-comment">#TCPKeepAlive yes</span><br><span class="hljs-comment">#UseLogin no</span><br><span class="hljs-comment">#UsePrivilegeSeparation sandbox</span><br>PermitUserEnvironment <span class="hljs-literal">no</span><br><span class="hljs-comment">#Compression delayed</span><br>ClientAliveInterval 600<br>ClientAliveCountMax 0<br><span class="hljs-comment">#ShowPatchLevel no</span><br><span class="hljs-comment">#UseDNS yes</span><br><span class="hljs-comment">#PidFile /var/run/sshd.pid</span><br><span class="hljs-comment">#MaxStartups 10:30:100</span><br><span class="hljs-comment">#PermitTunnel no</span><br><span class="hljs-comment">#ChrootDirectory none</span><br><span class="hljs-comment">#VersionAddendum none</span><br><br><span class="hljs-comment"># no default banner path</span><br>Banner /etc/issue.net<br><br><span class="hljs-comment"># Accept locale-related environment variables</span><br>AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES<br>AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT<br>AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE<br>AcceptEnv XMODIFIERS<br><br><span class="hljs-comment"># override default of no subsystems</span><br>Subsystemsftp/usr/libexec/openssh/sftp-server<br><br><span class="hljs-comment"># Example of overriding settings on a per-user basis</span><br><span class="hljs-comment">#Match User anoncvs</span><br><span class="hljs-comment">#X11Forwarding no</span><br><span class="hljs-comment">#AllowTcpForwarding no</span><br><span class="hljs-comment">#PermitTTY no</span><br><span class="hljs-comment">#ForceCommand cvs server</span><br>Ciphers 隐藏<br><span class="hljs-comment"># DenyUsers root</span><br><span class="hljs-comment"># DenyGroups root</span><br>AllowTcpForwarding <span class="hljs-literal">no</span><br>MaxStartups 10:30:60<br>MaxSessions 4<br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Cluster Deployment</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从0开始部署一个Flink集群：实践篇（独立部署）</title>
    <link href="/2022/12/18/Flink%E9%83%A8%E7%BD%B2%E7%8B%AC%E7%AB%8B%E9%83%A8%E7%BD%B2%E7%AF%87/"/>
    <url>/2022/12/18/Flink%E9%83%A8%E7%BD%B2%E7%8B%AC%E7%AB%8B%E9%83%A8%E7%BD%B2%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<p>服务器操作系统：centos7<br>本机操作系统：Mac<br>flink version: 1.15<br>jdk version: java11<br>HA service: Zookeeper<br>File System: NFS<br>资源分配：</p><table><thead><tr><th>ip</th><th>hostname</th><th>role</th></tr></thead><tbody><tr><td>10.250.0.1</td><td>main0</td><td>JM</td></tr><tr><td>10.250.0.2</td><td>main1</td><td>JM</td></tr><tr><td>10.250.0.3</td><td>main2</td><td>JM</td></tr><tr><td>10.250.0.4</td><td>worker1</td><td>TM</td></tr><tr><td>10.250.0.5</td><td>worker2</td><td>TM</td></tr></tbody></table><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>远程登录工具：iTerm<br>远程登录5台服务器，⌘(command) + ⇧(shift) + i快捷键同时操作它们。若登录账号非root账号，建议切换为root账号</p><h2 id="关闭SeLinux"><a href="#关闭SeLinux" class="headerlink" title="关闭SeLinux"></a>关闭SeLinux</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">setenforce 0</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">sed -i --follow-symlinks <span class="hljs-string">&#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27;</span> /etc/sysconfig/selinux</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">reboot</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">sestatus</span><br></code></pre></td></tr></table></figure><h2 id="配置hostname"><a href="#配置hostname" class="headerlink" title="配置hostname"></a>配置hostname</h2><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs accesslog">$ hostnamectl set-hostname main0 #main1,main2,worker1~<span class="hljs-number">2</span><br>$ hostnamectl status<br>$ vi /etc/hosts <br>$ cat /etc/hosts<br><span class="hljs-number">127.0.0.1</span> localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::<span class="hljs-number">1</span> localhost localhost.localdomain localhost6 localhost6.localdomain6<br><span class="hljs-number">10.250.0.1</span>: main0<br><span class="hljs-number">10.250.0.2</span>: main1<br><span class="hljs-number">10.250.0.3</span>: main2<br><span class="hljs-number">10.250.0.4</span>: worker1<br><span class="hljs-number">10.250.0.5</span>: worker2<br></code></pre></td></tr></table></figure><h2 id="同步时间"><a href="#同步时间" class="headerlink" title="同步时间"></a>同步时间</h2><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>yum install ntp -y<br><span class="hljs-variable">$ </span>crontab -e<br><span class="hljs-variable">$ </span>crontab -l<br><span class="hljs-number">0</span> * * * * <span class="hljs-regexp">/usr/sbin</span><span class="hljs-regexp">/ntpdate cn.pool.ntp.org</span><br></code></pre></td></tr></table></figure><h2 id="配置ssh免密登录"><a href="#配置ssh免密登录" class="headerlink" title="配置ssh免密登录"></a>配置ssh免密登录</h2><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-comment"># 1. 生成密钥</span><br><span class="hljs-variable">$ </span>ssh-keygen<br><span class="hljs-comment"># 2. 复制各自的公钥到其它服务器的authorized_key</span><br><span class="hljs-variable">$ </span>ssh-copy-id -i ~<span class="hljs-regexp">/.ssh/id</span>_rsa.pub main0 <span class="hljs-comment">#main1,main2,worker1~2</span><br></code></pre></td></tr></table></figure><p>关于复制公钥涉及到服务器之间的相互访问，可能会要求你输入root账号的密码，如果不知道可以通过手工复制粘贴代替ssh-copy-id命令。<br>之后会单独写一篇博文介绍ssh免密登录的理论知识和我遇到过的各种问题。</p><h2 id="安装Java11"><a href="#安装Java11" class="headerlink" title="安装Java11"></a>安装Java11</h2><p>安装flink之前需要装好java，1.15之后的版本只支持java11，不再支持java8.</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">$ yum search <span class="hljs-keyword">java|grep </span><span class="hljs-keyword">jdk</span><br><span class="hljs-keyword"></span>$ yum <span class="hljs-keyword">install </span>-y <span class="hljs-keyword">java-11-openjdk</span><br><span class="hljs-keyword"></span>$ yum <span class="hljs-keyword">install </span><span class="hljs-keyword">java-11-openjdk-devel </span>-y<br>$ <span class="hljs-keyword">java </span>-version<br></code></pre></td></tr></table></figure><h1 id="Zookeeper-HA-服务环境准备"><a href="#Zookeeper-HA-服务环境准备" class="headerlink" title="Zookeeper HA 服务环境准备"></a>Zookeeper HA 服务环境准备</h1><h2 id="Zookeeper集群搭建"><a href="#Zookeeper集群搭建" class="headerlink" title="Zookeeper集群搭建"></a>Zookeeper集群搭建</h2><p>Standalone部署模式下的flink集群只支持zookeeper高可用服务，所以若要部署Jobmanager高可用，必须部署zookeeper。<br>我选择将所有安装包都安装在&#x2F;opt目录下<br>zookeeper只涉及到前三台服务器，因此以下操作仅用于前三台服务器（main0,main1,main2)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> /opt</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">wget https://dlcdn.apache.org/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz --no-check-certificate</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">tar -xzf apache-zookeeper-3.7.1-bin.tar.gz<span class="hljs-string">&#x27;</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-string">重命名</span></span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-string">mv apache-zookeeper-3.7.1 zookeeper</span></span><br></code></pre></td></tr></table></figure><p>进入解压后的目录，修改配置文件zoo.cfg（默认只有zoo_sample.cfg，zoo.cfg需要自建）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># The number of milliseconds of each tick</span><br><span class="hljs-string">tickTime=2000</span><br><span class="hljs-comment"># The number of ticks that the initial</span><br><span class="hljs-comment"># synchronization phase can take</span><br><span class="hljs-string">initLimit=10</span><br><span class="hljs-comment"># The number of ticks that can pass between</span><br><span class="hljs-comment"># sending a request and getting an acknowledgement</span><br><span class="hljs-string">syncLimit=5</span><br><span class="hljs-comment"># the directory where the snapshot is stored.</span><br><span class="hljs-comment"># do not use /tmp for storage, /tmp here is just</span><br><span class="hljs-comment"># example sakes.</span><br><span class="hljs-string">dataDir=/opt/zookeeper/data</span><br><span class="hljs-string">dataLogDir=/opt/zookeeper/logs</span><br><span class="hljs-comment"># the port at which the clients will connect</span><br><span class="hljs-string">clientPort=2181</span><br><span class="hljs-comment"># the maximum number of client connections.</span><br><span class="hljs-comment"># increase this if you need to handle more clients</span><br><span class="hljs-comment">#maxClientCnxns=60</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Be sure to read the maintenance section of the</span><br><span class="hljs-comment"># administrator guide before turning on autopurge.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># The number of snapshots to retain in dataDir</span><br><span class="hljs-comment">#autopurge.snapRetainCount=3</span><br><span class="hljs-comment"># Purge task interval in hours</span><br><span class="hljs-comment"># Set to &quot;0&quot; to disable auto purge feature</span><br><span class="hljs-comment">#autopurge.purgeInterval=1</span><br><br><span class="hljs-comment">## Metrics Providers</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># https://prometheus.io Metrics Exporter</span><br><span class="hljs-comment">#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider</span><br><span class="hljs-comment">#metricsProvider.httpPort=7000</span><br><span class="hljs-comment">#metricsProvider.exportJvmInfo=true</span><br><span class="hljs-string">server.1=10.250.0.1:2888:3888</span><br><span class="hljs-string">server.2=10.250.0.2:2888:3888</span><br><span class="hljs-string">server.3=10.250.0.3:2888:3888</span><br></code></pre></td></tr></table></figure><p>进入配置文件中定义的dataDir目录，创建myid文件，在myid文件中写入各自的id。配置文件中server.1&#x3D;10.250.0.1:2888:3888里面的server.1 1就是id号。因此myid文件分别写入1，2，3即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> /opt/zookeeper/data</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">vi myid</span><br>  1  # 三台机器不一样，其它为2，3<br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">nohup</span> bin/zkServer.sh start &gt;&gt; /dev/null 2&gt;&amp;1 &amp;</span><br></code></pre></td></tr></table></figure><p>启动后可通过 ps -aux|grep zkServer 命令查看进程是否启动成功。<br>也可以通过jps命令查看是否有QuorumPeerMain进程<br>启动失败可以进入&#x2F;opt&#x2F;zookeeper&#x2F;logs查看对应的log。我这边的失败原因有：myid配置目录和zoo.cfg文件中的dataDir不一致；端口被占用。</p><h2 id="NFS-配置"><a href="#NFS-配置" class="headerlink" title="NFS 配置"></a>NFS 配置</h2><p>之所以配置NFS是因为配置Flink的zookeeper HA 服务需要配置可共享的文件存储（High-availability.storageDir must be a durable file system that is accessible from all nodes）官方示例给的hdfs，但是考虑到hdfs太笨重，我们的任务量也不大，我选择了更轻量的NFS。我的NFS服务是直接向同事申请的，同事提供了远程目录，我直接挂载到我的三台jobmanager服务器的&#x2F;mnt&#x2F;flink&#x2F;ha&#x2F;位置，之后这个目录就将配置为flink中的High-availability.storageDir</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">rpm -qa|grep nfs</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">rpm -qa|grep rpc</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-keyword">if</span> not installed, install nfs</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">yum -y install nfs-utils rpcbind</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">verify</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">rpm -qa nfs-utils rpcbind</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">moutn <span class="hljs-built_in">local</span> <span class="hljs-built_in">dir</span> to remote <span class="hljs-built_in">dir</span></span><br>mount -t nfs -o nolock 远程目录 /mnt/flink/ha/<br></code></pre></td></tr></table></figure><h1 id="Flink-集群部署与配置"><a href="#Flink-集群部署与配置" class="headerlink" title="Flink 集群部署与配置"></a>Flink 集群部署与配置</h1><p>下面操作对象为所有服务器</p><h2 id="下载并解压"><a href="#下载并解压" class="headerlink" title="下载并解压"></a>下载并解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> /opt</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">wget https://dlcdn.apache.org/flink/flink-1.15.2/flink-1.15.2-bin-scala_2.12.tgz --no-check-certificate</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">tar -xzf flink-*.tgz</span><br></code></pre></td></tr></table></figure><h2 id="Configure-masters-in-conf-x2F-masters"><a href="#Configure-masters-in-conf-x2F-masters" class="headerlink" title="Configure masters in conf&#x2F;masters:"></a>Configure masters in conf&#x2F;masters:</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">main0</span>:<span class="hljs-number">8081</span><br><span class="hljs-attribute">main1</span>:<span class="hljs-number">8081</span><br><span class="hljs-attribute">main2</span>:<span class="hljs-number">8081</span><br></code></pre></td></tr></table></figure><h2 id="Configure-workers-in-conf-x2F-workers"><a href="#Configure-workers-in-conf-x2F-workers" class="headerlink" title="Configure workers in conf&#x2F;workers:"></a>Configure workers in conf&#x2F;workers:</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">worker1</span><br>worker2<br></code></pre></td></tr></table></figure><h2 id="配置-conf-x2F-flink-conf-yaml"><a href="#配置-conf-x2F-flink-conf-yaml" class="headerlink" title="配置 conf&#x2F;flink-conf.yaml"></a>配置 conf&#x2F;flink-conf.yaml</h2><p>完整文件太长，不同机器不完全一样，下面仅列出worker1的配置项（去除了注释）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">jobmanager.rpc.address:</span> <span class="hljs-string">localhost</span><br><span class="hljs-attr">jobmanager.rpc.port:</span> <span class="hljs-number">6123</span><br><span class="hljs-attr">jobmanager.bind-host:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><br><span class="hljs-attr">jobmanager.memory.process.size:</span> <span class="hljs-string">1600m</span><br><span class="hljs-attr">taskmanager.bind-host:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><br><span class="hljs-attr">taskmanager.host:</span> <span class="hljs-string">worker1(服务器worker2上写worker2)</span><br><span class="hljs-attr">taskmanager.memory.process.size:</span> <span class="hljs-string">8192m</span><br><span class="hljs-attr">taskmanager.memory.managed.size:</span> <span class="hljs-string">0m</span><br><span class="hljs-attr">taskmanager.numberOfTaskSlots:</span> <span class="hljs-number">8</span><br><span class="hljs-attr">parallelism.default:</span> <span class="hljs-number">1</span><br><span class="hljs-attr">high-availability:</span> <span class="hljs-string">zookeeper</span><br><span class="hljs-attr">high-availability.storageDir:</span> <span class="hljs-string">file:///mnt/flink/ha/</span><br><span class="hljs-attr">high-availability.zookeeper.quorum:</span> <span class="hljs-string">main0:2181,main1:2181,main2:2181</span><br><span class="hljs-attr">high-availability.zookeeper.path.root:</span> <span class="hljs-string">/opt/flink-1.15.2/cluster_nodes</span><br><span class="hljs-attr">high-availability.cluster-id:</span> <span class="hljs-string">/cluster_one</span><br><span class="hljs-attr">jobmanager.execution.failover-strategy:</span> <span class="hljs-string">region</span><br><span class="hljs-attr">rest.address:</span> <span class="hljs-string">localhost</span><br><span class="hljs-attr">rest.bind-address:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure><h2 id="启动和关闭集群"><a href="#启动和关闭集群" class="headerlink" title="启动和关闭集群"></a>启动和关闭集群</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># <span class="hljs-keyword">start</span> <span class="hljs-keyword">cluster</span><br>$ bin/<span class="hljs-keyword">start</span>-<span class="hljs-keyword">cluster</span>.sh<br># <span class="hljs-keyword">close</span> <span class="hljs-keyword">cluster</span><br>$ bin/stop-<span class="hljs-keyword">cluster</span>.sh<br></code></pre></td></tr></table></figure><p>开启集群后可以在10.250.0.1：8081上看到UI界面<br><img src="/2022/12/18/Flink%E9%83%A8%E7%BD%B2%E7%8B%AC%E7%AB%8B%E9%83%A8%E7%BD%B2%E7%AF%87/2022-12-19-17-48-28.png"></p><p>启动集群后也可以关闭具体的某个taskmanager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">bin/taskmanager.sh stop</span><br></code></pre></td></tr></table></figure><p>同一台机器可以启动多个taskmanager（前提是资源够）</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell"><span class="hljs-variable">$</span> bin/taskmanager.sh <span class="hljs-built_in">start</span><br></code></pre></td></tr></table></figure><h1 id="Flink集群功能测试"><a href="#Flink集群功能测试" class="headerlink" title="Flink集群功能测试"></a>Flink集群功能测试</h1><p>关掉或kill掉一个taskmanager，可以看到运行的任务重启在另一个taskmanager上。<br>关掉或kill掉当前的jobmanager leader，可以在另一个jobmanager的 UI界面上监测到任务从恢复点恢复。</p>]]></content>
    
    
    <categories>
      
      <category>Flink</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
      <tag>Cluster Deployment</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
